<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>从零实现 Transformer：中英文翻译实例 - Laumy的技术栈</title>
    <link rel="stylesheet" href="assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="./">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="./">首页</a></div>
          <div class="nav-item site-link">
            <a href="https://www.laumy.tech" target="_blank" title="访问主站">主站点:www.laumy.tech</a>
          </div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#_1">概述</a><ul><li><a href="#_2">推理</a></li><li><a href="#_3">训练</a></li><li><a href="#_4">模型</a></li><li><a href="#_5">编解码器</a></li></ul></li><li><a href="#_6">数据预处理</a><ul><li><a href="#_7">数据准备</a></li><li><a href="#_8">数据加载器</a></li></ul></li><li><a href="#_9">模型架构</a><ul><li><a href="#_10">因果掩码</a></li><li><a href="#_11">前向传播</a></li><li><a href="#_12">解码推理</a></li><li><a href="#_13">位置编码</a></li></ul></li><li><a href="#_14">编码器</a><ul></ul></li><li><a href="#_15">解码器</a><ul></ul></li><li><a href="#_16">注意力</a><ul><li><a href="#multiheadattention">MultiHeadAttention</a></li><li><a href="#scaleddotproductattention">ScaledDotProductAttention</a></li></ul></li><li><a href="#_17">应用</a><ul></ul></li><li><a href="#_18">常见问题</a><ul></ul></li><li><a href="#_19">附：完整源码</a><ul></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>从零实现 Transformer：中英文翻译实例</h1>
  <div class="meta">
    <span class="meta-item">
      <i class="icon">🕒</i>
      2025-08-13
    </span>
    <span class="meta-item">
      <i class="icon">📂</i>
      ai
    </span>
    <span class="meta-item">
      <i class="icon">👤</i>
      laumy
    </span>
  </div>
  <div class="post-content"><h2 id="_1">概述</h2>
<p>在<a href="http://www.laumy.tech/2458.html#h37">http://www.laumy.tech/2458.html#h37</a>章节中，介绍了transformer的原理，本章用pytorch来实现一个将"我有一个苹果"翻译为英文"I have an apple"的模型，直观体会transformer原理实现。</p>
<p>接下来先上图看看整体的代码流程。</p>
<h3 id="_2">推理</h3>
<p><a href="http://www.laumy.tech/wp-content/uploads/2025/08/wp_editor_md_6d80929099229cfcce1c73395a7b334c.jpg"><img alt="" src="assets/doc/04-ai/ai应用/从零实现-transformer：中英文翻译实例/images/wp_editor_md_6d80929099229cfcce1c73395a7b334c.jpg"/></a></p>
<h3 id="_3">训练</h3>
<p><a href="http://www.laumy.tech/wp-content/uploads/2025/08/wp_editor_md_fccc127efa2c693fd129b29be8e431fc.jpg"><img alt="" src="assets/doc/04-ai/ai应用/从零实现-transformer：中英文翻译实例/images/wp_editor_md_fccc127efa2c693fd129b29be8e431fc.jpg"/></a></p>
<h3 id="_4">模型</h3>
<p><a href="http://www.laumy.tech/wp-content/uploads/2025/08/wp_editor_md_dcb1f4c849f582a89fef2fc0976838df.jpg"><img alt="" src="assets/doc/04-ai/ai应用/从零实现-transformer：中英文翻译实例/images/wp_editor_md_dcb1f4c849f582a89fef2fc0976838df.jpg"/></a></p>
<h3 id="_5">编解码器</h3>
<p><a href="http://www.laumy.tech/wp-content/uploads/2025/08/wp_editor_md_68ec4fb52b99c13063748e4ac35ec513.jpg"><img alt="" src="assets/doc/04-ai/ai应用/从零实现-transformer：中英文翻译实例/images/wp_editor_md_68ec4fb52b99c13063748e4ac35ec513.jpg"/></a></p>
<p>到这里就涵盖了整个transformer模型翻译的例子了，下面的章节只是对图中的代码进行展开说明，如果不想陷入细节，可以直接跳转到最后一节获取源码运行实验一下。</p>
<h2 id="_6">数据预处理</h2>
<h3 id="_7">数据准备</h3>
<p><strong>（1） 准备原始文本对</strong></p>
<p>既然要做翻译那得先有数据用于模型训练，因此需要先准备原始的中文-&gt;英文的文本对，下面是使用python列表(List)准备中英匹配语料，List中包含的是元组(Tuple)。</p>
<div class="codehilite"><pre><span></span><code><span class="n">pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"i have an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 有 一本 书"</span><span class="p">,</span> <span class="s2">"i have a book"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"你 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"you have an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"他 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"he has an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"她 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"she has an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我们 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"we have an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 喜欢 苹果"</span><span class="p">,</span> <span class="s2">"i like apples"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 吃 苹果"</span><span class="p">,</span> <span class="s2">"i eat apples"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"你 喜欢 书"</span><span class="p">,</span> <span class="s2">"you like books"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 喜欢 书"</span><span class="p">,</span> <span class="s2">"i like books"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 有 两个 苹果"</span><span class="p">,</span> <span class="s2">"i have two apples"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 有 红色 苹果"</span><span class="p">,</span> <span class="s2">"i have red apples"</span><span class="p">),</span>
<span class="p">]</span>
</code></pre></div>
<p>为了方便，在构建原始文本对时，中英文的分词就以空格划分，这样接下来就可以根据空格来进行构建词表。</p>
<p><strong>（2）构建词表</strong></p>
<p>因为神经网络不能直接处理文本，模型只能处理数字，比如不能直接处理"我"、"有"，"I"等中英文词，对于计算机来讲都是数字，所以需要把文字转换为对应的映射表。 所以词表就是一个"字典"，把每个词映射到一个唯一的数字ID上，所有的文本都需要转换为数字序列。</p>
<p>如下示例，中英文的编号。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 中文词表示例</span>
<span class="n">SRC_STOI</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"我"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"有"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> 
    <span class="s2">"一个"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">"苹果"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">"书"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">"喜欢"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="c1"># ... 更多词</span>
<span class="p">}</span>

<span class="c1"># 英文词表示例</span>
<span class="n">TGT_STOI</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"i"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">"have"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">"an"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">"apple"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">"a"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">"book"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="c1"># ... 更多词</span>
<span class="p">}</span>
</code></pre></div>
<p>如何构建词表了。既然中文、英文都需要各自编号，那么得先把此前准备的原始文本队中文、英文各自拆出来，然后我们使用python的set集合，将中文、英文分别添加到set集合中，使用set集合的好处是可以自动去重，添加了重复元素，set就不会添加，这样就得到了各自的中文、英文词表。最后再对这些词表进行依次编号即可。</p>
<p>下面就看看使用python代码怎么实现，首先是将原始文本对拆解，把中文放一起，英文放一起。</p>
<div class="codehilite"><pre><span></span><code><span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
<span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">src_texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tgt_texts</span><span class="p">)</span>

<span class="n">src_texts</span> <span class="p">[</span><span class="s1">'我 有 一个 苹果'</span><span class="p">,</span> <span class="s1">'我 有 一本 书'</span><span class="p">,</span> <span class="s1">'你 有 一个 苹果'</span><span class="p">,</span> <span class="s1">'他 有 一个 苹果'</span><span class="p">,</span> <span class="s1">'她 有 一个 苹果'</span><span class="p">,</span> <span class="s1">'我们 有 一个 苹果'</span><span class="p">,</span> <span class="s1">'我 喜欢 苹果'</span><span class="p">,</span> <span class="s1">'我 吃 苹果'</span><span class="p">,</span> <span class="s1">'你 喜欢 书'</span><span class="p">,</span> <span class="s1">'我 喜欢 书'</span><span class="p">,</span> <span class="s1">'我 有 两个 苹果'</span><span class="p">,</span> <span class="s1">'我 有 红色 苹果'</span><span class="p">]</span>
<span class="n">tgt_texts</span> <span class="p">[</span><span class="s1">'i have an apple'</span><span class="p">,</span> <span class="s1">'i have a book'</span><span class="p">,</span> <span class="s1">'you have an apple'</span><span class="p">,</span> <span class="s1">'he has an apple'</span><span class="p">,</span> <span class="s1">'she has an apple'</span><span class="p">,</span> <span class="s1">'we have an apple'</span><span class="p">,</span> <span class="s1">'i like apples'</span><span class="p">,</span> <span class="s1">'i eat apples'</span><span class="p">,</span> <span class="s1">'you like books'</span><span class="p">,</span> <span class="s1">'i like books'</span><span class="p">,</span> <span class="s1">'i have two apples'</span><span class="p">,</span> <span class="s1">'i have red apples'</span><span class="p">]</span>
</code></pre></div>
<p>接下来实现一个build_vocab函数，主要的思路就是句子先按照空格进行分好词，接着将所有词添加到set集合中，set集合会自动去重，这里需要注意的时，需要再加上3个特殊的词，分别是pad、bos、eos分别表示填充、开始、结束。填充是因为输入句子是不定长的，但是对于transformer来说所有的输入矩阵处理都是固定长度，所以不够的需要补齐，而bos和eos是用于transformer解码的，便于开始和结束翻译过程，最后构建好词表后就按照词表中进行变化，3个特殊词分为为1、2、3其他的词依次编号。</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">"""构建词表（字符串→索引 与 索引→字符串）</span>
<span class="sd">    - 输入示例为用空格分词后的句子列表</span>
<span class="sd">    - 加入特殊符号 `&lt;pad&gt;`, `&lt;bos&gt;`, `&lt;eos&gt;` 并将其它 token 排序，保证可复现</span>
<span class="sd">    返回：</span>
<span class="sd">      stoi: dict[token-&gt;id]</span>
<span class="sd">      itos: List[id-&gt;token]</span>
<span class="sd">    """</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># 建立一个集合，用于存储所有的词表（不重复的词）</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span> <span class="c1"># 依次遍历获得每个句子</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">():</span> <span class="c1"># 通过空格划分，依次遍历句子中的每个词，</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="c1"># 将词添加到set中，这里为了方便统一转换小写</span>
    <span class="n">itos</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"&lt;pad&gt;"</span><span class="p">,</span> <span class="s2">"&lt;bos&gt;"</span><span class="p">,</span> <span class="s2">"&lt;eos&gt;"</span><span class="p">]</span> <span class="o">+</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="c1"># 加入3个特殊的词，同时对set中的词进行排序。</span>
    <span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">t</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">itos</span><span class="p">)}</span> <span class="c1"># 对词表中的词按照顺序依次编号</span>
    <span class="k">return</span> <span class="n">stoi</span><span class="p">,</span> <span class="n">itos</span>

<span class="n">SRC_STOI</span><span class="p">,</span> <span class="n">SRC_ITOS</span> <span class="o">=</span> <span class="n">build_vocab</span><span class="p">(</span><span class="n">src_texts</span><span class="p">)</span>
<span class="n">TGT_STOI</span><span class="p">,</span> <span class="n">TGT_ITOS</span> <span class="o">=</span> <span class="n">build_vocab</span><span class="p">(</span><span class="n">tgt_texts</span><span class="p">)</span>
</code></pre></div>
<p>build_vocab最终返回是一个字典和列表，字典是词:编号的映射，列表是存放的是词表。列表是按照编号顺序依次排布，这样我们可以通过编号定位到时那个词。</p>
<p>为什么要一个字典和列表了？因为transformer输入是词-&gt;编号（转换为编码数字给计算机处理），输出是编号-&gt;词过程（转化为句子给人看）。通过字典我们可以查询词对应的编号[key:value]，而通过列表的索引（编号）我们可以查询到对应的词。</p>
<p>中文和英文分别各自对应一个字典和词表。</p>
<div class="codehilite"><pre><span></span><code><span class="n">SRC_STOI</span> <span class="p">{</span><span class="s1">'&lt;pad&gt;'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">'&lt;bos&gt;'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'&lt;eos&gt;'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'一个'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'一本'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">'两个'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">'书'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">'他'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="s1">'你'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">'吃'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'喜欢'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">'她'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="s1">'我'</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s1">'我们'</span><span class="p">:</span> <span class="mi">13</span><span class="p">,</span> <span class="s1">'有'</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'红色'</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="s1">'苹果'</span><span class="p">:</span> <span class="mi">16</span><span class="p">}</span>

<span class="n">SRC_ITOS</span> <span class="p">[</span><span class="s1">'&lt;pad&gt;'</span><span class="p">,</span> <span class="s1">'&lt;bos&gt;'</span><span class="p">,</span> <span class="s1">'&lt;eos&gt;'</span><span class="p">,</span> <span class="s1">'一个'</span><span class="p">,</span> <span class="s1">'一本'</span><span class="p">,</span> <span class="s1">'两个'</span><span class="p">,</span> <span class="s1">'书'</span><span class="p">,</span> <span class="s1">'他'</span><span class="p">,</span> <span class="s1">'你'</span><span class="p">,</span> <span class="s1">'吃'</span><span class="p">,</span> <span class="s1">'喜欢'</span><span class="p">,</span> <span class="s1">'她'</span><span class="p">,</span> <span class="s1">'我'</span><span class="p">,</span> <span class="s1">'我们'</span><span class="p">,</span> <span class="s1">'有'</span><span class="p">,</span> <span class="s1">'红色'</span><span class="p">,</span> <span class="s1">'苹果'</span><span class="p">]</span>

<span class="n">TGT_STOI</span> <span class="p">{</span><span class="s1">'&lt;pad&gt;'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">'&lt;bos&gt;'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'&lt;eos&gt;'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">'an'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">'apple'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">'apples'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">'book'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="s1">'books'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">'eat'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">'has'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">'have'</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span> <span class="s1">'he'</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">:</span> <span class="mi">13</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="s1">'she'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">'two'</span><span class="p">:</span> <span class="mi">17</span><span class="p">,</span> <span class="s1">'we'</span><span class="p">:</span> <span class="mi">18</span><span class="p">,</span> <span class="s1">'you'</span><span class="p">:</span> <span class="mi">19</span><span class="p">}</span>

<span class="n">TGT_ITOS</span> <span class="p">[</span><span class="s1">'&lt;pad&gt;'</span><span class="p">,</span> <span class="s1">'&lt;bos&gt;'</span><span class="p">,</span> <span class="s1">'&lt;eos&gt;'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'an'</span><span class="p">,</span> <span class="s1">'apple'</span><span class="p">,</span> <span class="s1">'apples'</span><span class="p">,</span> <span class="s1">'book'</span><span class="p">,</span> <span class="s1">'books'</span><span class="p">,</span> <span class="s1">'eat'</span><span class="p">,</span> <span class="s1">'has'</span><span class="p">,</span> <span class="s1">'have'</span><span class="p">,</span> <span class="s1">'he'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">,</span> <span class="s1">'she'</span><span class="p">,</span> <span class="s1">'two'</span><span class="p">,</span> <span class="s1">'we'</span><span class="p">,</span> <span class="s1">'you'</span><span class="p">]</span>
</code></pre></div>
<p>这样我们就给中文和英文的所有词都编好号了，同时通过列表也可以通过编号查询到词。</p>
<h3 id="_8">数据加载器</h3>
<p>在pytorch中模型训练那必然少不了DataLoader和Dataset，关于这两个类的介绍在<a href="http://www.laumy.tech/2491.html#h23">http://www.laumy.tech/2491.html#h23</a>中有简要说明，这里就不阐述了。注意本小节说明的数据的批量处理都适用于训练准备，主要是实现Dataset和Dataloader用于pytorch模型的训练，如果只是推理则是不需要的。</p>
<p><strong>（1）Dataset继承类实现</strong></p>
<p>首先要实现DataLoader中关键的输入类Dataset继承类，用于产出“单个样本”，怎么按索引取到一个样本，以及总共有多少个样本。每个样本是中文句子-&gt;英文句子。样本集为此前定义pairs，但是要把pairs中句子转换为编号，词表在前面我们已经构建好了，直接查询就行，那这里我们定义一个Example用于定义样本，src是中文句子的编号列表，tgt是对于英文句子的编号列表。</p>
<div class="codehilite"><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Example</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""单条并行样本</span>
<span class="sd">    - src: 源语言索引序列（不含 BOS/EOS）</span>
<span class="sd">    - tgt: 目标语言索引序列（含 BOS/EOS）</span>
<span class="sd">    """</span>
    <span class="n">src</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">tgt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
</code></pre></div>
<p>接下来就是实现Dataset的继承类ToyDataset，返回有多少个样本，以及通过编号获取指定的样本。</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ToyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""语料数据集，用于快速过拟合演示。"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">Example</span><span class="p">(</span><span class="n">encode_src</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">encode_tgt</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div>
<p>需要把pairs句子中词列表编码为数字列表，这里实现encode_src用于将输入（即pairs中的中文）编号为列表，再实现encode_tgt将输出（即pairs中的英文）编号为列表。使用for列表推导式从pairs列表中获取到s(中文句子)和t(英文句子)然后传入encode_src和encoder_tgt进而构建一个新的列表元素Example。这样就组建样本的self.data的样本列表，元素为Example类型，可以通过idx获取到指定的样本。</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">encode_src</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""将原语句（已空格分词）编码为索引序列（不含 BOS/EOS）。"""</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">SRC_STOI</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>

<span class="k">def</span> <span class="nf">encode_tgt</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""将目标语句编码为索引序列，并在首尾添加 BOS/EOS。"""</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">BOS_IDX</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">TGT_STOI</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_IDX</span><span class="p">]</span>
</code></pre></div>
<p>上面就是输入句子编码为编号向量的实现了，也很简单，通过此前构建的词表字典，通过词就可以搜索到对应编号了。这里需要注意的是编码的源句子（输入）是没有包含BOS和EOS的，因为transformer的编码器不需要BOS和EOS，而编码的目标句子（输出）需要在句子前加上BOS，句子结尾加上EOS，因为transformer的解码器输入需要通过BOS来翻译第一个词，通过EOS来结束一个句子的翻译，要是不明白为什么了可以看看前面transformer原理的文章。</p>
<p><strong>（2）Dataload</strong></p>
<p>DataLoader 负责“成批取样”，模型训练输入数据不是一个样本一个样本的送入训练，而是按照批次（多个样本合成一个批次）进行训练，这样训练效率才高。DataLoader决定批大小、是否打乱、多进程加载，返回的是一个可迭代的对象。</p>
<p>DataLoader重点是要实现 collate_fn回调，也就是怎么把一个批里的样本“拼起来”。</p>
<div class="codehilite"><pre><span></span><code><span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">)</span>
</code></pre></div>
<p>训练transformer，准备数据。我们的目的是要能够返回批量数据，批量数据也有好几个类型。</p>
<ul>
<li>输入给encoder批量数据：输入矩阵类型(B,S)，包含补齐的padding。</li>
<li>输入给decoder的批量数据：输入给decoder的矩阵类型(B,T)，包含BOS以及右对齐的padding。不能加EOS，因为EOS是预测的结果，防止模型训练作弊。</li>
<li>decoder输出的批量数据：解码器的监督目标，主要用于预测数据与实际的结果比较计算损失，矩阵类型(B,T)，不含BOS但是包含EOS。</li>
<li>encoder输入的pad掩码数据：因为输入给encoder的数据有padding，所以要告诉transformer哪些做了补齐，后续计算的时候要处理。</li>
<li>decoder输入的pad掩码数据：同上。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Example</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">"""将一个 batch 的样本对齐为等长张量，并构造 teacher forcing 所需的输入/输出。</span>
<span class="sd">    返回：</span>
<span class="sd">      - src: (B,S) 源序列，已 padding</span>
<span class="sd">      - tgt_in: (B,T) 解码器输入（含 BOS，右对齐 padding）</span>
<span class="sd">      - tgt_out: (B,T) 解码器监督目标（对 tgt_in 右移一位，含 EOS）</span>
<span class="sd">      - src_pad_mask: (B,S) 源端 padding 掩码，True 表示 padding 位置</span>
<span class="sd">      - tgt_pad_mask: (B,T) 目标端 padding 掩码（针对输入序列）</span>
<span class="sd">    """</span>
    <span class="c1"># padding to max length in batch</span>
    <span class="n">src_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">src</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">tgt_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">tgt</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>

    <span class="n">src_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_in_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_out_batch</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD_IDX</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">src_max</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">src</span><span class="p">))</span>
        <span class="c1"># Teacher forcing: shift-in, shift-out</span>
        <span class="n">tgt_in</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD_IDX</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">tgt_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD_IDX</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">tgt_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

        <span class="n">src_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt_in_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_in</span><span class="p">)</span>
        <span class="n">tgt_out_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_out</span><span class="p">)</span>

    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>         <span class="c1"># (B, S)</span>
    <span class="n">tgt_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_in_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>   <span class="c1"># (B, T_in)</span>
    <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_out_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="c1"># (B, T_out)</span>
    <span class="n">src_pad_mask</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>                          <span class="c1"># (B, S)</span>
    <span class="n">tgt_pad_mask</span> <span class="o">=</span> <span class="n">tgt_in</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>                       <span class="c1"># (B, T)</span>
    <span class="k">return</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">tgt_out</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span>
</code></pre></div>
<p>上面就是Dataloader回调函数如何获取批量数据的实现了，输入为一个列表（包含所有样本的列表）。输出为5个2维向量，分别对应的就是上面说的5个批量数据。</p>
<p>首先计算样本列表中最长的源序列长度src_max和目标序列长度tgt_max，为后续的不足长度的句子进行padding操作，提供基准的长度。</p>
<p>其次使用for循环遍历每个样本（Example），将源序列src（encoder的输入）使用PAD_IDX填充到相同长度，保持做对齐；将目标序列输入(tgt_in)去掉最后一个token(EOS)作为decoder的输入，目标序列输出比对样本tgb_out去掉第一个tokenBOS作为监督目标，使用的teacher Forcing机制，这样就是实现了输入预测下一个的训练模式数据准备。</p>
<p>最后就是准备src和tgt_in的mask矩阵，形状跟src和tgt_in一样，使用python的eq比对如果对应的位置是padding就是true，不是就是false。</p>
<h2 id="_9">模型架构</h2>
<p>数据准备好了，接下来就是设计我们的模型了。我们的模型是一个翻译模型可以分为两个路径，一个是编码路径和解码路径。</p>
<ul>
<li>编码路径：词嵌入-&gt;位置编码-&gt;编码器。</li>
<li>解码路径：词嵌入-&gt;位置编码-&gt;解码器-&gt;生成器。</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">Class</span> <span class="n">Seq2SeqTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                 <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="c1"># 编码路径</span>
        <span class="c1">#  1.词嵌入层，将tokenID转换为密集向量</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_tok</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">PAD_IDX</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_tok</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">PAD_IDX</span><span class="p">)</span>
        <span class="c1">#  2. 对输入添加位置信息</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="c1">#  3. 源序列的编码</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="c1"># 解码路径</span>
        <span class="c1">#  1. 解码生成目标序列</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="c1">#  2. 将解码器输出转换为词表概率</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
</code></pre></div>
<p>词嵌入直接调用的是神经网络的库nn.Embedding，其他部分都要自己实现，接下来我们会一一展开。下面我们需要先实现模型Seq2SeqTransformer的方法，主要包括如下：</p>
<ul>
<li>make_subsequent_mask：解码器因果掩码，不允许解码器看到未来。</li>
<li>forward: 模型前向传播的方法，pytorch训练的时候自动调用。</li>
<li>greedy_decode：模型推理方法，用于推理的应用。</li>
</ul>
<h3 id="_10">因果掩码</h3>
<p>为什么需要掩码了？主要是让模型不能看到未来的词。</p>
<p>推理阶段虽然是自回归一个一个输入然后一个一个迭代输出，但是在训练阶段，我们解码器的样本是全部一次性输入的。如下的步骤，我们虽然给到模型输入为："BOS i have an apple "，但是每个步骤给到模型看到的不能是全部，否则给模型都看到输入结果了，那还谈啥预测，模型会偷懒直接就照搬就是一个映射过程了。如当输入BOS i 期望预测输出i have，如果没有掩码模型都看到全部的"BOS i have an apple "，就不是预测了，模型的参数也没法迭代了。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 步骤1: 输入BOS → 期望输出i</span>
<span class="c1"># 步骤2: 输入BOS i → 期望输出i have</span>
<span class="c1"># 步骤3: 输入BOS i have → 期望输出i have an</span>
<span class="c1"># 步骤4: 输入BOS i have an → 期望输出 i have an apple</span>
<span class="c1"># 步骤5: 输入BOS i have an apple → 期望输出i have an apple EOS</span>
</code></pre></div>
<p>哪有个问题，为什么我们输入的时候不按照要多少输入多少，为啥要全部一下给到输入？输入倒是可以要多少输入多少，但是要要考虑模型的并行训练，实际上上面的5个步骤在模型训练时是并行进行的，模型训练要的是训练参数，在某个阶段看到什么输入遇到什么输出，都分好类了自然可以并行的，所以这就需要结合掩码了，告诉模型那个步骤你能看到哪些？</p>
<p>总结一下mask的作用就是让模型不能看到未来的词，同时也是让模型不要对padding位进行误预测。</p>
<div class="codehilite"><pre><span></span><code>    <span class="k">def</span> <span class="nf">make_subsequent_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sz</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""构造大小为 (sz, sz) 的下三角因果掩码；True 为屏蔽（不允许看未来）。"""</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<p>mask是要生成一个下三角形状，示例如下：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 对于序列长度4</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">make_subsequent_mask</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># 结果：</span>
<span class="c1"># [[False,  True,  True,  True],   # 位置0: 只能看位置0</span>
<span class="c1">#  [False, False,  True,  True],   # 位置1: 能看位置0,1</span>
<span class="c1">#  [False, False, False,  True],   # 位置2: 能看位置0,1,2</span>
<span class="c1">#  [False, False, False, False]]   # 位置3: 能看所有位置</span>
</code></pre></div>
<h3 id="_11">前向传播</h3>
<div class="codehilite"><pre><span></span><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""训练/教师强制阶段的前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - src: (B, S) 源 token id</span>
<span class="sd">          - tgt_in: (B, T) 目标端输入（以 BOS 开头）</span>
<span class="sd">          - src_pad_mask: (B, S) True 为 padding</span>
<span class="sd">          - tgt_pad_mask: (B, T) True 为 padding（针对 tgt_in）</span>
<span class="sd">        返回：</span>
<span class="sd">          - logits: (B, T, V) 词表维度的分类分布</span>
<span class="sd">        """</span>
        <span class="c1"># 1) 词嵌入 + 位置编码</span>
        <span class="n">src_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_tok</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>  <span class="c1"># (B,S,C)</span>
        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_tok</span><span class="p">(</span><span class="n">tgt_in</span><span class="p">))</span>  <span class="c1"># (B,T,C)</span>

        <span class="c1"># 2) 编码：仅使用 key_padding_mask 屏蔽 padding</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_emb</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">)</span>  <span class="c1"># (B,S,C)</span>

        <span class="c1"># 3) 解码：自注意力需要因果掩码 + padding 掩码；交叉注意力需要 memory 的 padding 掩码</span>
        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_subsequent_mask</span><span class="p">(</span><span class="n">tgt_in</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># (T,T)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
            <span class="n">tgt_emb</span><span class="p">,</span>
            <span class="n">memory</span><span class="p">,</span>
            <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">,</span>
            <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_pad_mask</span><span class="p">,</span>
            <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># (B,T,C)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logits</span>
</code></pre></div>
<p>上面就是模型的训练了，也比较简单，就是对输入词进行词嵌入+位置编码计算，然后送入编码器得到输出特征矩阵memory；给编码器输入的只是padding的掩码，因为不要提取padding的词；</p>
<p>其次生成因果掩码，将编码器的的特征矩阵输出结果memory以及解码器侧自身的输入给到解码器最终得到(B,T,C)的输出矩阵，其包含了最终输出结果词位置的隐藏信息；</p>
<p>最后调用self.generator(out)即线性变化得到输出目标词表的概率分布(B,T,V)；后面就可以用其使用交叉熵跟目标结果进行比对计算损失了。</p>
<h3 id="_12">解码推理</h3>
<div class="codehilite"><pre><span></span><code>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">greedy_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""在推理阶段进行贪心解码。</span>
<span class="sd">        参数：</span>
<span class="sd">          - src_ids: 源端 token id 序列（不含 BOS/EOS）</span>
<span class="sd">          - max_len: 最大生成长度（含 BOS/EOS）</span>
<span class="sd">          - device: 运行设备</span>
<span class="sd">        返回：</span>
<span class="sd">          - 生成的目标端 id 序列（含 BOS/EOS）</span>
<span class="sd">        """</span>
        <span class="c1">#切换为评估模式，关闭dropout/batchnorm等随机性</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># 将源端token id序列转换为张量，并添加一个维度，如[1, 2, 3, 4] -&gt; [[1, 2, 3, 4]]</span>
        <span class="c1"># 变为批维度的 (1, S)；dtype 为 long 主要是以适配 nn.Embedding的输入格式。</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 生成一个跟src相同形状的mask矩阵，让编码器不要计算提取pandding的位置信息。</span>
        <span class="c1">#按元素判断 src 是否等于 PAD_IDX，等于的位置为 True，不等的位置为 False。</span>
        <span class="n">src_pad_mask</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>

        <span class="c1"># 计算src_tok= src 经过词嵌入+位置编码后的结果</span>
        <span class="n">src_tok</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_tok</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">src_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="n">src_tok</span><span class="p">)</span>

        <span class="c1"># 将该结果送入编码器,返回的memory就是编码器提取的特征向量。</span>
        <span class="c1"># 输入编码器，即使没有填充(pandding)的token,也需要传入src_key_padding_mask。</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_pos</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">)</span>

        <span class="c1"># 初始化目标端token id序列，维度为(1,1)，初始值为BOS_IDX</span>
        <span class="c1"># 表示目标端序列的开始，BOS_IDX=1</span>
        <span class="c1"># 推理时输入是没有PAD，但是仍然需要tgt_pad_mask.</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">BOS_IDX</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1">#计算本次解码的Mask，跟ys形状一样。</span>
            <span class="n">tgt_pad_mask</span> <span class="o">=</span> <span class="n">ys</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>
            <span class="c1"># 计算本次因果掩码，把未来看到的token都屏蔽。</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_subsequent_mask</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># 可以看到当推理模式时，解码器输入token数量依次是1，2，3，4.....</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_tok</span><span class="p">(</span><span class="n">ys</span><span class="p">)),</span>
                <span class="n">memory</span><span class="p">,</span>
                <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_pad_mask</span><span class="p">,</span>
                <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># 转化为预测词的概率分布</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>

            <span class="c1"># 使用贪心选择概率最大的作为本次预测的目标</span>
            <span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">next_id</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># 显示选择的token</span>
            <span class="n">token_text</span> <span class="o">=</span> <span class="n">TGT_ITOS</span><span class="p">[</span><span class="n">next_id</span><span class="p">]</span> <span class="k">if</span> <span class="n">next_id</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">TGT_ITOS</span><span class="p">)</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">"ID_</span><span class="si">{</span><span class="n">next_id</span><span class="si">}</span><span class="s2">"</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"选择: </span><span class="si">{</span><span class="n">token_text</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">next_id</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>

            <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ys</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># 当下一个输出为EOS时表示结束，则退出。</span>
            <span class="k">if</span> <span class="n">next_id</span> <span class="o">==</span> <span class="n">EOS_IDX</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="n">ys</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</code></pre></div>
<p>上面代码的设计要点主要为几个部分：</p>
<ul>
<li>编码信息提取：将要翻译的句子进行词嵌入，位置编码，然后送入编码器计算提出特征信息memory，最终给到解码器作为输入。</li>
<li>自回归生成：最开始使用BOS一个token+编码器此前计算的输出memory、掩码等信息输入给解码器，解码器预测得到一个输出，然后将输出拼接会此前BOS的后面形成解码器新的输入，以此循环进行预测，直至遇到EOS结束。解侧输入序列长度逐步增长：1 → 2 → 3 → 4 → ...，最开始的序列为BOS表示开始。</li>
<li>掩码生成：使用了因果掩码和padding掩码；虽然推理阶段没有对输入数据进行padding操作，但是依旧需要这两个掩码，主要的考量是保持接口的一致性（原来的接口需要传递这个参数）。</li>
<li>贪心策略：解码器的输出进行线性变化得到词表的概率分布后，然后挑选概率最高的token。</li>
<li>结束循环：当判断到模型预测出EOS时，模式则结束，整个预测完成。</li>
</ul>
<h3 id="_13">位置编码</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""经典正弦/余弦位置编码。</span>
<span class="sd">    给定嵌入 `x (B,L,C)`，按长度切片并与位置编码相加，再做 dropout。</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="c1"># 创建一个形状为 (max_len, d_model) 的零张量，用于存储位置编码</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (L, C)</span>
        <span class="c1"># 创建一个形状为 (max_len, 1) 的张量，用于存储位置索引</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (L, 1)</span>
        <span class="c1"># 创建一个形状为 (d_model//2,) 的张量，用于存储位置编码的缩放因子</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="c1"># sin, cos 交错</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, C)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>  <span class="c1"># (B, L, C)</span>
<span class="w">        </span><span class="sd">"""为输入嵌入添加位置编码并做 dropout。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, L, C)</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, L, C)</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 对于位置 pos 和维度 i：</span>
<span class="c1"># 偶数维度: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</span>
<span class="c1"># 奇数维度: PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</span>

<span class="c1"># pe[:, 0::2]: 选择所有行的偶数列 (0, 2, 4, ...)</span>
<span class="c1"># pe[:, 1::2]: 选择所有行的奇数列 (1, 3, 5, ...)</span>

<span class="c1"># 计算过程：</span>
<span class="c1"># 位置0: sin(0 * div_term), cos(0 * div_term), sin(0 * div_term), ...</span>
<span class="c1"># 位置1: sin(1 * div_term), cos(1 * div_term), sin(1 * div_term), ...</span>
<span class="c1"># 位置2: sin(2 * div_term), cos(2 * div_term), sin(2 * div_term), ...</span>
</code></pre></div>
<p>位置编码比较简单，就是按照sin和cos按公式计算生成向量，最终返回词嵌入向量+位置编码向量。</p>
<h2 id="_14">编码器</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""堆叠若干编码层。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, S, C)</span>
<span class="sd">          - src_key_padding_mask: (B, S) True 为 padding</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, S, C)</span>
<span class="sd">        """</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>编码器框架就是若干个编码层堆叠起来，但是每层的都有自己的参数，主要调用的是nn.ModuleList进行注册子模块，确保参数都能够被优化器找到，num_layers控制了编码器的深度。</p>
<p>前向传播函数也很简单，输入一次通过每一个编码层，得到的输出结果给到下一个编码层，以此循环最终经过最后一层编码器得得到的特征信息，给后续解码器使用。</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Transformer 编码层（后归一化 post-norm 版本）</span>
<span class="sd">    子层：自注意力 + 前馈；均带残差连接与 LayerNorm。</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""单层编码层前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, S, C)</span>
<span class="sd">          - src_key_padding_mask: (B, S) True 为 padding</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, S, C)</span>
<span class="sd">        """</span>
        <span class="c1"># 自注意力子层</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_out</span><span class="p">)</span>
        <span class="c1"># 前馈子层</span>
        <span class="n">ff_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>编码层的组件为MultiHeadAttention、LayerNorm、PositionwiseFeedForward这与我们此前介绍的transformer原理一致。</p>
<p>其前向传播过程，首先输入X（查询），X（键），X（值），qkv都是一样的；注意力计算时，把attn_mask=None，因为编码器不需要因果掩码，但是需要padding mask。其次进行残差连接计算x+attn_out，再调用norml进行层归一化，最后是计算前馈网络，再进行归一化就得到一层的输出结果了。</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""前馈网络：逐位置的两层 MLP（含激活与 dropout）"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""两层逐位置前馈网络。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, L, C)</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, L, C)</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>前馈网络主要两层：</p>
<ul>
<li>第一层：d_model → dim_ff (通常 dim_ff = 4 * d_model)</li>
<li>激活函数：ReLU。</li>
<li>第二层：dim_ff → d_model</li>
</ul>
<p>就是对输入进行升维然后非线性变化再降维，提取更多的信息。两层都使用了dropout，展开就是如下。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 第一层线性变换</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                    <span class="c1"># (B, L, C) → (B, L, dim_ff)</span>

<span class="c1"># 2. 激活函数</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                    <span class="c1"># 应用ReLU</span>

<span class="c1"># 3. 第一个dropout</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                <span class="c1"># 随机置零部分神经元</span>

<span class="c1"># 4. 第二层线性变换</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                    <span class="c1"># (B, L, dim_ff) → (B, L, C)</span>

<span class="c1"># 5. 第二个dropout</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                <span class="c1"># 最终dropout</span>
</code></pre></div>
<h2 id="_15">解码器</h2>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">memory</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">tgt_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memory_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""堆叠若干解码层。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, T, C) 目标端嵌入</span>
<span class="sd">          - memory: (B, S, C) 编码器输出</span>
<span class="sd">          - tgt_mask: (T, T) 因果掩码，True 为屏蔽</span>
<span class="sd">          - tgt_key_padding_mask: (B, T) 目标端 padding 掩码</span>
<span class="sd">          - memory_key_padding_mask: (B, S) 源端 padding 掩码</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, T, C)</span>
<span class="sd">        """</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">memory</span><span class="p">,</span>
                <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">,</span>
                <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_key_padding_mask</span><span class="p">,</span>
                <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">memory_key_padding_mask</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>与编码器类似，使用nn.ModuleList创建多个解码层，每个解码层都是独立的DecoderLayer实例；解码器的输入数据有两个，一个是解码器侧自己的输入序列，另外一个是编码器计算得到的特征信息。解码器的每一层都需要输入编码器给的特征序列，但是都是一样的；解码器层计算得到的输出将传递给下一层解码器层，循环得到最后的输出。</p>
<div class="codehilite"><pre><span></span><code><span class="n">Decoder</span> <span class="p">(</span><span class="n">解码器</span><span class="p">)</span>
<span class="err">├──</span> <span class="n">DecoderLayer</span> <span class="mi">1</span> <span class="p">(</span><span class="n">解码层1</span><span class="p">)</span>
<span class="err">│</span>   <span class="err">├──</span> <span class="n">MultiHeadAttention</span> <span class="p">(</span><span class="n">自注意力</span><span class="p">)</span>
<span class="err">│</span>   <span class="err">├──</span> <span class="n">LayerNorm1</span> <span class="o">+</span> <span class="n">残差连接</span>
<span class="err">│</span>   <span class="err">├──</span> <span class="n">MultiHeadAttention</span> <span class="p">(</span><span class="n">交叉注意力</span><span class="p">)</span>
<span class="err">│</span>   <span class="err">├──</span> <span class="n">LayerNorm2</span> <span class="o">+</span> <span class="n">残差连接</span>
<span class="err">│</span>   <span class="err">├──</span> <span class="n">PositionwiseFeedForward</span> <span class="p">(</span><span class="n">前馈网络</span><span class="p">)</span>
<span class="err">│</span>   <span class="err">└──</span> <span class="n">LayerNorm3</span> <span class="o">+</span> <span class="n">残差连接</span>
<span class="err">├──</span> <span class="n">DecoderLayer</span> <span class="mi">2</span> <span class="p">(</span><span class="n">解码层2</span><span class="p">)</span>
<span class="err">│</span>   <span class="err">└──</span> <span class="o">...</span> <span class="p">(</span><span class="n">同上结构</span><span class="p">)</span>
<span class="err">└──</span> <span class="o">...</span> <span class="p">(</span><span class="n">重复</span> <span class="n">num_layers</span> <span class="n">次</span><span class="p">)</span>

<span class="n">输入</span><span class="p">:</span> <span class="n">x</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="o">+</span> <span class="n">memory</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="err">→</span> <span class="n">DecoderLayer</span> <span class="mi">1</span> <span class="err">→</span> <span class="n">DecoderLayer</span> <span class="mi">2</span> <span class="err">→</span> <span class="o">...</span> <span class="err">→</span> <span class="n">DecoderLayer</span> <span class="n">N</span> <span class="err">→</span> <span class="n">输出</span><span class="p">:</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div>
<p>其前向传播也大同小异，与编码器不同的是需要传递因果掩码，tgt_mask，防止看到未来信息，同时还传入了源序列的pandding掩码，跟输入给编码器的mask是一样的。</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Transformer 解码层（自注意力 + 交叉注意力 + 前馈）"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">memory</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">tgt_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memory_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""单层解码层前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, T, C) 解码器输入</span>
<span class="sd">          - memory: (B, S, C) 编码器输出</span>
<span class="sd">          - tgt_mask: (T, T) 因果掩码,true为屏蔽</span>
<span class="sd">          - tgt_key_padding_mask: (B, T)</span>
<span class="sd">          - memory_key_padding_mask: (B, S)</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, T, C)</span>
<span class="sd">        """</span>
        <span class="c1"># 1) 解码器自注意力（带因果掩码 tgt_mask）</span>
        <span class="n">sa</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">tgt_key_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">sa</span><span class="p">)</span>
        <span class="c1"># 2) 交叉注意力：Q 来自解码器，K/V 来自编码器 memory</span>
        <span class="n">ca</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">memory_key_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ca</span><span class="p">)</span>
        <span class="c1"># 3) 前馈</span>
        <span class="n">ff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p>解码器层比编码器层多了一个cross_attn交叉注意力。除了输入数据有些不同，其他都基本类似，下面按前向传播的流程来分析一下。</p>
<p>首先是第一个子层自注意力的计算，输入X(q),X(k),X(v)来自解码器侧路径的输入，推理模式则是由自己预测自回归的输入，训练模式是给定的。自注意力传入了因果掩码attn_mask和屏蔽pandding mask。</p>
<p>其次就是计算残差和层归一化，与编码器类似。</p>
<p>接着就是计算交叉注意力了，核心的注意力类还是MultiHeadAttention，跟编码器和解码器的都来自一个。唯一的区别就是传入的参数不一样，其中查询Q来自于解码器当前的状态X即解码器上一个自注意力的的输出，特征路径是解码器给的信息。而键值K，V则使用的是编码器的输出memory，不使用因果掩码，因为因果掩码前面已经处理了。</p>
<p>最后就是前馈网络的升维和降维处理等了，跟编码器就一样了，就不阐述了。</p>
<p>三个子层的不同作用：</p>
<ul>
<li>自注意力层：处理目标序列内部的关系，生成"i have an apple"时，"have"应该关注"i"，"an"应该关注"i have"，通过因果掩码确保只能看到历史信息。</li>
<li>交叉注意力层：让解码器"看到"编码器的信息，翻译成英文时，需要参考中文源序列，通过交叉注意力，解码器可以访问编码器的完整表示。</li>
<li>前馈网络则层：增加非线性表达能力，每个位置独立计算，不涉及位置间的关系。</li>
</ul>
<h2 id="_16">注意力</h2>
<p>接下来就是核心MultiHeadAttention。</p>
<h3 id="multiheadattention">MultiHeadAttention</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""多头注意力（Batch-first）</span>
<span class="sd">    - 输入输出为 (B, L, C)</span>
<span class="sd">    - 内部将通道 C 切分到 H 个头，每头维度 Dh=C/H</span>
<span class="sd">    - 支持两类掩码：</span>
<span class="sd">        1) attn_mask: (Lq, Lk) 下三角等自回归掩码</span>
<span class="sd">        2) key_padding_mask: (B, Lk) 序列 padding 掩码</span>
<span class="sd">    两者会在内部合并为可广播到 (B,H,Lq,Lk) 的布尔张量。</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">nhead</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"d_model 必须能被 nhead 整除"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nhead</span> <span class="o">=</span> <span class="n">nhead</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">nhead</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># 将 (B, L, C) 重塑为 (B, L, H, Dh)，原来的数据都不会变化，只是形状改变了</span>
    <span class="c1"># 加了一个维，然后交换了张量维度顺序。</span>
    <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""(B, L, C) 切分重排为 (B, H, L, Dh)。"""</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># 第一步：将 (B, L, C) 重塑为 (B, L, H, Dh)</span>
        <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nhead</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="c1">#x.view不复制数据，只是改变数据的"视角"，数据在内存中存储顺序不变</span>

        <span class="c1"># 第二步：交换维度 1 和 2，从 (B, L, H, Dh) 变为 (B, H, L, Dh)</span>
        <span class="n">x_transposed</span> <span class="o">=</span> <span class="n">x_reshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_transposed</span>

    <span class="k">def</span> <span class="nf">_merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""(B, H, L, Dh) 合并重排回 (B, L, C)。"""</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">Dh</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># 第一步：交换维度 1 和 2，从 (B, H, L, Dh) 变为 (B, L, H, Dh)</span>
        <span class="n">x_transposed</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># 第二步：确保内存连续，然后重塑为 (B, L, H*Dh)</span>
        <span class="n">x_contiguous</span> <span class="o">=</span> <span class="n">x_transposed</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1"># 第三步：重塑为 (B, L, C) 其中 C = H * Dh</span>
        <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x_contiguous</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span> <span class="o">*</span> <span class="n">Dh</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_reshaped</span>

    <span class="c1"># 因为QKV算的是矩阵，在transformer中涉及到两个mask</span>
    <span class="c1"># 一个是attn_mask控制哪些位置可以相互关注，如因果掩码防止看未来</span>
    <span class="c1"># 一个是key_padding_mask控制哪些位置是有效的，如填充token不应该被关注</span>
    <span class="c1"># 因为都要计算所以把这两个使用|合并起来，一起跟QKV计算即可，否则得计算两次。</span>
    <span class="c1"># 对于encode来说传参只会穿key_pandding_mask，另外一个没有</span>
    <span class="c1"># 对于decoder来说，两个都会传递。</span>
    <span class="k">def</span> <span class="nf">_build_attn_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">Lq</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">Lk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""将两类掩码合并成 (1/ B, 1/ H, Lq, Lk) 可广播布尔张量。True 表示屏蔽。"""</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># (Lq, Lk) -&gt; (1,1,Lq,Lk)</span>
            <span class="n">m1</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">m1</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">mask</span> <span class="o">|</span> <span class="n">m1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># (B, Lk) -&gt; (B,1,1,Lk)</span>
            <span class="n">m2</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">m2</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">mask</span> <span class="o">|</span> <span class="n">m2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span>
</code></pre></div>
<p><strong>（0）网络层定义</strong></p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">w_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># 查询线性变换</span>
<span class="bp">self</span><span class="o">.</span><span class="n">w_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># 键线性变换</span>
<span class="bp">self</span><span class="o">.</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># 值线性变换</span>
<span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># 缩放点积注意力</span>
<span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># 输出投影</span>
<span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>        <span class="c1"># 输出dropout</span>
</code></pre></div>
<p>w_q, w_k, w_v: 将输入转换为查询、键、值表示，attn为计算注意力权重和加权求和，proj将多头结果投影会原始维度，dropout是防止过拟合。</p>
<p><strong>（1）将输入分成多个头</strong></p>
<p>对输入按照head划分为多份，所以这里需要注意的是d_model必现要能被nhead整除，确保每个头有相同的维度。如原来的输入为(B,L,C)切分后变成(B, H, L, Dh)，Dh=d_model/nhead。</p>
<p>第一步先使用view重塑为(B, H, L, Dh)，然后第二步进行重排。举个例子输入为(B, L, C) = (1, 4, 6)重塑为(B, L, H, Dh) = (1, 4, 2, 3)，重塑后的内存布局，[word1_head1_3, word1_head2_3, word2_head1_3, word2_head2_3, ...]每个词的头是交错存储的，为了适应多头注意力的并行计算还要重排一下，让每个头的数据连续存储。</p>
<p><strong>（2）掩码合并</strong></p>
<p>将key_padding_mask和attn_mask(因果)进行合并，这样后续计算就不用计算两次了。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用逻辑或运算 | 合并</span>
<span class="c1"># True | True = True (屏蔽)</span>
<span class="c1"># True | False = True (屏蔽)</span>
<span class="c1"># False | False = False (不屏蔽)</span>

<span class="c1"># 最终掩码形状: (B, H, Lq, Lk) 或 (1, H, Lq, Lk)</span>
<span class="c1"># 可以广播到注意力计算的形状</span>
</code></pre></div>
<p><a href="http://www.laumy.tech/wp-content/uploads/2025/08/wp_editor_md_c35b08ef53dccaf7a547e2f523262e69.jpg"><img alt="" src="assets/doc/04-ai/ai应用/从零实现-transformer：中英文翻译实例/images/wp_editor_md_c35b08ef53dccaf7a547e2f523262e69.jpg"/></a></p>
<p><strong>（3）每个头计算注意力</strong></p>
<div class="codehilite"><pre><span></span><code>        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_q</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>  <span class="c1"># (B,H,Lq,Dh)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_k</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>    <span class="c1"># (B,H,Lk,Dh)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_v</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># (B,H,Lk,Dh)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_attn_mask</span><span class="p">(</span><span class="n">Lq</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Dh)</span>
</code></pre></div>
<p>计算注意力时，首先对输入分别进行计算线性变换（如QxWq，这样就有参数了）然后重排分别得到QKV，对于编码器来说输入的query、key、value都是一样的，计算QKV的方式也是一样的，都是进行线性nn.Linear层然后再进行重排，但是各自有各自参数，这就是要训练的参数。经过线性层的结果后都需要调用_shape进行重排划分为多个头的数据，便于输入给多头注意力；构建好合并后的掩码之后，就传递到attn中计算注意力。计算出的多头的注意力，需要合并为原来的形状，最后再通过一个线性变化得到最后的结果输出。</p>
<p>完整的数据流示例：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 输入: query (1, 4, 6), key (1, 4, 6), value (1, 4, 6)</span>
<span class="c1"># 参数: d_model=6, nhead=2, d_head=3</span>

<span class="c1"># 步骤1: 线性变换 (保持形状)</span>
<span class="c1"># w_q(query): (1, 4, 6) -&gt; (1, 4, 6)</span>
<span class="c1"># w_k(key): (1, 4, 6) -&gt; (1, 4, 6)  </span>
<span class="c1"># w_v(value): (1, 4, 6) -&gt; (1, 4, 6)</span>

<span class="c1"># 每个词从6维变换到6维</span>
<span class="c1"># 学习查询、键、值的表示</span>

<span class="c1"># 步骤2: 分头</span>
<span class="c1"># _shape(w_q(query)): (1, 4, 6) -&gt; (1, 2, 4, 3)</span>
<span class="c1"># _shape(w_k(key)): (1, 4, 6) -&gt; (1, 2, 4, 3)</span>
<span class="c1"># _shape(w_v(value)): (1, 4, 6) -&gt; (1, 2, 4, 3)</span>

<span class="c1"># 将6维分成2个头，每个头3维</span>
<span class="c1"># 头1: 3维表示</span>
<span class="c1"># 头2: 3维表示</span>

<span class="c1"># 步骤3: 注意力计算</span>
<span class="c1"># attn(Q, K, V, mask): (1, 2, 4, 3) -&gt; (1, 2, 4, 3)</span>

<span class="c1"># 每个头独立计算注意力：</span>
<span class="c1"># 头1: 计算4个位置之间的注意力，每个位置3维</span>
<span class="c1"># 头2: 计算4个位置之间的注意力，每个位置3维</span>

<span class="c1"># 步骤4: 合并头</span>
<span class="c1"># _merge(out): (1, 2, 4, 3) -&gt; (1, 4, 6)</span>

<span class="c1"># 将2个头的3维表示合并回6维</span>
<span class="c1"># 每个位置现在包含所有头的信息</span>

<span class="c1"># 步骤5: 输出变换</span>
<span class="c1"># proj(out): (1, 4, 6) -&gt; (1, 4, 6)</span>
<span class="c1"># dropout(out): (1, 4, 6) -&gt; (1, 4, 6)</span>

<span class="c1"># 最终输出: (1, 4, 6)</span>
</code></pre></div>
<h3 id="scaleddotproductattention">ScaledDotProductAttention</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""缩放点积注意力（单头）</span>
<span class="sd">    给定 Q(查询)、K(键)、V(值) 与掩码，计算注意力加权输出。</span>
<span class="sd">    形状约定：</span>
<span class="sd">      - Q: (B, H, Lq, Dh)</span>
<span class="sd">      - K: (B, H, Lk, Dh)</span>
<span class="sd">      - V: (B, H, Lk, Dh)</span>
<span class="sd">      - mask: 可广播到 (B, H, Lq, Lk)，True 表示屏蔽。</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""计算缩放点积注意力。</span>
<span class="sd">        参数：</span>
<span class="sd">          - Q: (B, H, Lq, Dh)</span>
<span class="sd">          - K: (B, H, Lk, Dh)</span>
<span class="sd">          - V: (B, H, Lk, Dh)</span>
<span class="sd">          - mask: 可广播到 (B, H, Lq, Lk) 的布尔掩码，True 表示屏蔽</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, H, Lq, Dh)</span>
<span class="sd">        """</span>
        <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 注意力分数 = QK^T / sqrt(dk)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Lk)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># 对被屏蔽位置填充一个极小值，softmax 后 ~0</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"-inf"</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Lk)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Dh)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
<p>这里就是实现缩放点积注意力机制了，Q.transpose(-2, -1)将K的最后两个维度转置，torch.matmul(Q, K^T): 计算Q和K的点积，再math.sqrt(d_k): 缩放因子，防止分数过大。</p>
<p>可以看到会根据传入的mask进行处理，让mask=True的位置会被填充为-inf，这样经过softmax之后，这些位置就接近0，从而实现了屏蔽某位位置的效果。</p>
<p>softmax是将分数转换为概率分布，所有位置的权重和为1，分数越高的位置，权重越大，也就是跟词相关性越大提取的值越丰富，如果是0那基本不相关，掩码为true的位置就是0，也就是基本不提取信息。</p>
<p>总结一下，核心就是公式Attention(Q,K,V) = softmax(QK^T/√d_k)V计算。</p>
<h2 id="_17">应用</h2>
<p>接下来就是调用应用了</p>
<div class="codehilite"><pre><span></span><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ToyDataset</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Seq2SeqTransformer</span><span class="p">(</span>
    <span class="n">src_vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">SRC_ITOS</span><span class="p">),</span>
    <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">TGT_ITOS</span><span class="p">),</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_IDX</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
</code></pre></div>
<p>定义dataset、loader准备数据，然后定义模型model，损失函数定义以及优化方法。</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate_sample</span><span class="p">(</span><span class="n">sent</span><span class="o">=</span><span class="s2">"我 有 一个 苹果"</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""辅助函数：对输入中文句子进行编码→推理→解码并打印结果。"""</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">encode_src</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"ids"</span><span class="p">,</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">pred_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">greedy_decode</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">pred_text</span> <span class="o">=</span> <span class="n">decode_tgt</span><span class="p">(</span><span class="n">pred_ids</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'INPUT : </span><span class="si">{</span><span class="n">sent</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'OUTPUT: </span><span class="si">{</span><span class="n">pred_text</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Before training:"</span><span class="p">)</span>
<span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">)</span>
</code></pre></div>
<p>上面是整个应用翻译应用，在没有训练出参数，自然预测出的结果是不对的。</p>
<div class="codehilite"><pre><span></span><code><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">800</span> <span class="c1"># 小步数即可过拟合玩具数据</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">tgt_out</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tgt_in</span> <span class="o">=</span> <span class="n">tgt_in</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">tgt_out</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">src_pad_mask</span> <span class="o">=</span> <span class="n">src_pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tgt_pad_mask</span> <span class="o">=</span> <span class="n">tgt_pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span><span class="p">)</span>  <span class="c1"># (B, T, V)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tgt_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2"> | loss=</span><span class="si">{</span><span class="n">total_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">)</span>
</code></pre></div>
<p>上面是训练过程。</p>
<h2 id="_18">常见问题</h2>
<p><strong>（1） 解码器训练时的输入和推理时的输入有什么不同？</strong></p>
<p>训练模式是固定长度输入，例如（2，5），所有样本都padding到相同长度，批次内所有样本的长度一致。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用教师强制，目标序列已知</span>
<span class="n">tgt_in</span> <span class="o">=</span> <span class="p">[</span><span class="n">BOS</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">have</span><span class="p">,</span> <span class="n">an</span><span class="p">,</span> <span class="n">apple</span><span class="p">,</span><span class="n">PAD</span><span class="p">]</span>     <span class="c1"># 完整的输入序列</span>
<span class="n">tgt_out</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">have</span><span class="p">,</span> <span class="n">an</span><span class="p">,</span> <span class="n">apple</span><span class="p">,</span> <span class="n">EOS</span><span class="p">]</span>     <span class="c1"># 完整的监督目标</span>
</code></pre></div>
<p>而推理模式序列长度随着时间步逐步增长，例如# 例如: (1, 1) → (1, 2) → (1, 3) → ...，每次生成后长度+1。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 逐步生成，每次只预测下一个token</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[[</span><span class="n">BOS_ID</span><span class="p">]]</span>                                    <span class="c1"># 第1步</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[[</span><span class="n">BOS_ID</span><span class="p">,</span> <span class="n">i</span><span class="p">]]</span>                                 <span class="c1"># 第2步</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[[</span><span class="n">BOS_ID</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">have</span><span class="p">]]</span>                       <span class="c1"># 第3步</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[[</span><span class="n">BOS_ID</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">have</span><span class="p">,</span> <span class="n">an</span><span class="p">]]</span>                 <span class="c1"># 第4步</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[[</span><span class="n">BOS_ID</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">have</span><span class="p">,</span> <span class="n">an</span><span class="p">,</span><span class="n">apple</span><span class="p">]]</span>       <span class="c1"># 第5步</span>
</code></pre></div>
<p>之所以有这样的差异是训练时用的是Teacher Forcing优势，使用了并行计算让所有位置可以同时计算预测，提高效率快速收敛。而推理时是自回归模式，每个token的生成只能基于之前输出的信息。</p>
<p><strong>（2）什么情况下输入数据需要PAD?</strong></p>
<p>通常无论是编码器的输入还是解码器的输入如果不是批量并行计算都可以不用PAD，但如果是批量并行都需要PAD MASK。</p>
<p>在训练模式下，为了提高效率需要批量并行计算，所以无论编码器还是解码器的输入都是需要PAD，在本文中要不要PAD动作是在DataLoader的回调函数中collate_fn进行的，会对编码器和解码器的输入都会pad对齐到一样的长度。</p>
<p>因此最主要的考量是否要批量并行计算，因为并行计算如果长度不同，无法并行处理，无论是自注意力分数、前馈网络、还是残差连接，只有长度一致，才能并行一下处理多个样本。而往往训练模型基本都是批量处理。</p>
<p>总之只处理一个样本时可以不需要PAD，如果要批量都一定需要PAD。而只处理一个样本，往往是推理模式场景。</p>
<p><strong>（3）既然推理模式的编码器和解码器输入没有进行PAD到一定长度，那为什么无论编码器和解码器都依旧还需要传入PAD mask？</strong></p>
<p>需要PAD mask我认为本质上有两点原因：其一用于告知模型输入序列的长度，其二为了接口的一致性，因为transformer最核心的是无论编码器还是解码器最终的核心是Scaled Dot-Product Attetion，可以理解为这是一个共有底层函数，都要调用，做兼容了所以一定要传这个参数。</p>
<p><strong>（3）推理模式的解码器既然是一个一个token往后生成的然后依次拼接回给到输入，未来的词其实根本就没有输入，为什么还需要下三角度的因果mask？</strong></p>
<p>本质上还是保证接口的兼容性，这块都无论是推理还是训练模式都需要传入这个因果mask。</p>
<p>首先在实现层面让训练模式和推理模式代码能够兼容，训练模式使用的是teacher forcing把整个目标序列一次性喂进去，那自然不能让模型看到未来token。推理模式严格上如果一次一个token，每次只输入已经生成的部分，在这种最简单的视线下，确实不需要再加下三角mask，因为未来token不存在，自然无法attend到。但是大多数框架都选择统一接口，无论训练还是推理都传causal mask，避免在不同模式下切换逻辑。</p>
<p>其次从推理模式的多样性考虑，即使是推理阶段，也有可能遇到这种情况，也就是批量生成，一次生成多个序列，每个序列长度不同。</p>
<p>下三角是一个通用的"未来屏蔽"机制，不只是为了防止模型看见未来token，也是为了让实现和训练推理保持一致，并支持批量/并行推理优化。</p>
<h2 id="_19">附：完整源码</h2>
<div class="codehilite"><pre><span></span><code><span class="c1"># toy_transformer_translation.py</span>
<span class="c1"># A tiny, runnable Transformer seq2seq example to translate Chinese-&gt;English on a toy dataset.</span>
<span class="c1"># PyTorch &gt;= 2.0 recommended.</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># --------------------------</span>
<span class="c1"># 1) Toy parallel corpus</span>
<span class="c1"># --------------------------</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># 基本陈述</span>
    <span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"i have an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 有 一本 书"</span><span class="p">,</span> <span class="s2">"i have a book"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"你 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"you have an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"他 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"he has an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"她 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"she has an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我们 有 一个 苹果"</span><span class="p">,</span> <span class="s2">"we have an apple"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 喜欢 苹果"</span><span class="p">,</span> <span class="s2">"i like apples"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 吃 苹果"</span><span class="p">,</span> <span class="s2">"i eat apples"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"你 喜欢 书"</span><span class="p">,</span> <span class="s2">"you like books"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 喜欢 书"</span><span class="p">,</span> <span class="s2">"i like books"</span><span class="p">),</span>
    <span class="c1"># 稍作扩展</span>
    <span class="p">(</span><span class="s2">"我 有 两个 苹果"</span><span class="p">,</span> <span class="s2">"i have two apples"</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">"我 有 红色 苹果"</span><span class="p">,</span> <span class="s2">"i have red apples"</span><span class="p">),</span>
<span class="p">]</span>

<span class="c1"># 中文使用"空格分词（简化）"，英文用空格分词</span>
<span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">"""构建词表（字符串→索引 与 索引→字符串）</span>
<span class="sd">    - 输入示例为用空格分词后的句子列表</span>
<span class="sd">    - 加入特殊符号 `&lt;pad&gt;`, `&lt;bos&gt;`, `&lt;eos&gt;` 并将其它 token 排序，保证可复现</span>
<span class="sd">    返回：</span>
<span class="sd">      stoi: dict[token-&gt;id]</span>
<span class="sd">      itos: List[id-&gt;token]</span>
<span class="sd">    """</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="c1"># 建立一个集合，用于存储所有不同的token</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span> <span class="c1"># 遍历所有句子，s是句子,如我 有 一个 苹果</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">():</span> <span class="c1"># 遍历句子中的每个token，t是token,如我</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="c1"># 将token添加到集合中，并转换为小写,如我</span>
    <span class="c1"># 特殊符号</span>
    <span class="n">itos</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"&lt;pad&gt;"</span><span class="p">,</span> <span class="s2">"&lt;bos&gt;"</span><span class="p">,</span> <span class="s2">"&lt;eos&gt;"</span><span class="p">]</span> <span class="o">+</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="c1"># 将特殊符号和所有不同的token排序</span>
    <span class="c1"># print(itos)</span>
    <span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">t</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">itos</span><span class="p">)}</span> <span class="c1"># 将token和索引建立映射关系</span>
    <span class="c1"># print(stoi)</span>
    <span class="k">return</span> <span class="n">stoi</span><span class="p">,</span> <span class="n">itos</span>

<span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
<span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"src_texts"</span><span class="p">,</span><span class="n">src_texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"tgt_texts"</span><span class="p">,</span><span class="n">tgt_texts</span><span class="p">)</span>
<span class="n">SRC_STOI</span><span class="p">,</span> <span class="n">SRC_ITOS</span> <span class="o">=</span> <span class="n">build_vocab</span><span class="p">(</span><span class="n">src_texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"SRC_STOI"</span><span class="p">,</span><span class="n">SRC_STOI</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"SRC_ITOS"</span><span class="p">,</span><span class="n">SRC_ITOS</span><span class="p">)</span>
<span class="n">TGT_STOI</span><span class="p">,</span> <span class="n">TGT_ITOS</span> <span class="o">=</span> <span class="n">build_vocab</span><span class="p">(</span><span class="n">tgt_texts</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TGT_STOI"</span><span class="p">,</span><span class="n">TGT_STOI</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TGT_ITOS"</span><span class="p">,</span><span class="n">TGT_ITOS</span><span class="p">)</span>

<span class="n">PAD_IDX</span><span class="p">,</span> <span class="n">BOS_IDX</span><span class="p">,</span> <span class="n">EOS_IDX</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="c1">#将源语句编码为索引序列（不含 BOS/EOS），如我 有 一个 苹果 -&gt; [1, 2, 3, 4]</span>
<span class="k">def</span> <span class="nf">encode_src</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""将原语句（已空格分词）编码为索引序列（不含 BOS/EOS）。"""</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">SRC_STOI</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>

<span class="k">def</span> <span class="nf">encode_tgt</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""将目标语句编码为索引序列，并在首尾添加 BOS/EOS。"""</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">BOS_IDX</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">TGT_STOI</span><span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_IDX</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">decode_tgt</span><span class="p">(</span><span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""将目标端索引序列解码回字符串（忽略 PAD/BOS，遇到 EOS 停止）。"""</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">EOS_IDX</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="n">PAD_IDX</span><span class="p">,</span> <span class="n">BOS_IDX</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TGT_ITOS</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Example</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""单条并行样本</span>
<span class="sd">    - src: 源语言索引序列（不含 BOS/EOS）</span>
<span class="sd">    - tgt: 目标语言索引序列（含 BOS/EOS）</span>
<span class="sd">    """</span>
    <span class="n">src</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">tgt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">ToyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""极小玩具平行语料数据集，用于快速过拟合演示。"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pairs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">Example</span><span class="p">(</span><span class="n">encode_src</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">encode_tgt</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Example</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">"""将一个 batch 的样本对齐为等长张量，并构造 teacher forcing 所需的输入/输出。</span>
<span class="sd">    返回：</span>
<span class="sd">      - src: (B,S) 源序列，已 padding</span>
<span class="sd">      - tgt_in: (B,T) 解码器输入（含 BOS，右对齐 padding）</span>
<span class="sd">      - tgt_out: (B,T) 解码器监督目标（对 tgt_in 右移一位，含 EOS）</span>
<span class="sd">      - src_pad_mask: (B,S) 源端 padding 掩码，True 表示 padding 位置</span>
<span class="sd">      - tgt_pad_mask: (B,T) 目标端 padding 掩码（针对输入序列）</span>
<span class="sd">    """</span>
    <span class="c1"># padding to max length in batch</span>
    <span class="n">src_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">src</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">tgt_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">tgt</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>

    <span class="n">src_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_in_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_out_batch</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD_IDX</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">src_max</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">src</span><span class="p">))</span>
        <span class="c1"># Teacher forcing: shift-in, shift-out</span>
        <span class="n">tgt_in</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD_IDX</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">tgt_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">PAD_IDX</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">tgt_max</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">tgt</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

        <span class="n">src_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt_in_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_in</span><span class="p">)</span>
        <span class="n">tgt_out_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_out</span><span class="p">)</span>

    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>         <span class="c1"># (B, S)</span>
    <span class="n">tgt_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_in_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>   <span class="c1"># (B, T_in)</span>
    <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_out_batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="c1"># (B, T_out)</span>
    <span class="n">src_pad_mask</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>                          <span class="c1"># (B, S)</span>
    <span class="n">tgt_pad_mask</span> <span class="o">=</span> <span class="n">tgt_in</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>                       <span class="c1"># (B, T)</span>
    <span class="k">return</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">tgt_out</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span>

<span class="c1"># --------------------------</span>
<span class="c1"># 2) Positional encoding</span>
<span class="c1"># --------------------------</span>
<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""经典正弦/余弦位置编码。</span>
<span class="sd">    给定嵌入 `x (B,L,C)`，按长度切片并与位置编码相加，再做 dropout。</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="c1"># 创建一个形状为 (max_len, d_model) 的零张量，用于存储位置编码</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (L, C)</span>
        <span class="c1"># 创建一个形状为 (max_len, 1) 的张量，用于存储位置索引</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (L, 1)</span>
        <span class="c1"># 创建一个形状为 (d_model//2,) 的张量，用于存储位置编码的缩放因子</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="c1"># sin, cos 交错</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, L, C)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>  <span class="c1"># (B, L, C)</span>
<span class="w">        </span><span class="sd">"""为输入嵌入添加位置编码并做 dropout。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, L, C)</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, L, C)</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># --------------------------</span>
<span class="c1"># 3) 手写 Transformer 编码/解码层（含详细注释）</span>
<span class="c1"># --------------------------</span>

<span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""缩放点积注意力（单头）</span>
<span class="sd">    给定 Q(查询)、K(键)、V(值) 与掩码，计算注意力加权输出。</span>
<span class="sd">    形状约定：</span>
<span class="sd">      - Q: (B, H, Lq, Dh)</span>
<span class="sd">      - K: (B, H, Lk, Dh)</span>
<span class="sd">      - V: (B, H, Lk, Dh)</span>
<span class="sd">      - mask: 可广播到 (B, H, Lq, Lk)，True 表示屏蔽。</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""计算缩放点积注意力。</span>
<span class="sd">        参数：</span>
<span class="sd">          - Q: (B, H, Lq, Dh)</span>
<span class="sd">          - K: (B, H, Lk, Dh)</span>
<span class="sd">          - V: (B, H, Lk, Dh)</span>
<span class="sd">          - mask: 可广播到 (B, H, Lq, Lk) 的布尔掩码，True 表示屏蔽</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, H, Lq, Dh)</span>
<span class="sd">        """</span>
        <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 注意力分数 = QK^T / sqrt(dk)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Lk)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># 对被屏蔽位置填充一个极小值，softmax 后 ~0</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"-inf"</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Lk)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Dh)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""多头注意力（Batch-first）</span>
<span class="sd">    - 输入输出为 (B, L, C)</span>
<span class="sd">    - 内部将通道 C 切分到 H 个头，每头维度 Dh=C/H</span>
<span class="sd">    - 支持两类掩码：</span>
<span class="sd">        1) attn_mask: (Lq, Lk) 下三角等自回归掩码</span>
<span class="sd">        2) key_padding_mask: (B, Lk) 序列 padding 掩码</span>
<span class="sd">    两者会在内部合并为可广播到 (B,H,Lq,Lk) 的布尔张量。</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">nhead</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"d_model 必须能被 nhead 整除"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nhead</span> <span class="o">=</span> <span class="n">nhead</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">nhead</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># 将 (B, L, C) 重塑为 (B, L, H, Dh)，原来的数据都不会变化，只是形状改变了</span>
    <span class="c1"># 加了一个维，然后交换了张量维度顺序。</span>
    <span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""(B, L, C) 切分重排为 (B, H, L, Dh)。"""</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># 第一步：将 (B, L, C) 重塑为 (B, L, H, Dh)</span>
        <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nhead</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="c1">#x.view不复制数据，只是改变数据的"视角"，数据在内存中存储顺序不变</span>

        <span class="c1"># 第二步：交换维度 1 和 2，从 (B, L, H, Dh) 变为 (B, H, L, Dh)</span>
        <span class="n">x_transposed</span> <span class="o">=</span> <span class="n">x_reshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_transposed</span>

    <span class="k">def</span> <span class="nf">_merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""(B, H, L, Dh) 合并重排回 (B, L, C)。"""</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">Dh</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># 第一步：交换维度 1 和 2，从 (B, H, L, Dh) 变为 (B, L, H, Dh)</span>
        <span class="n">x_transposed</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># 第二步：确保内存连续，然后重塑为 (B, L, H*Dh)</span>
        <span class="n">x_contiguous</span> <span class="o">=</span> <span class="n">x_transposed</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1"># 第三步：重塑为 (B, L, C) 其中 C = H * Dh</span>
        <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x_contiguous</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span> <span class="o">*</span> <span class="n">Dh</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_reshaped</span>

    <span class="c1"># 因为QKV算的是矩阵，在transformer中涉及到两个mask</span>
    <span class="c1"># 一个是attn_mask控制哪些位置可以相互关注，如因果掩码防止看未来</span>
    <span class="c1"># 一个是key_padding_mask控制哪些位置是有效的，如填充token不应该被关注</span>
    <span class="c1"># 因为都要计算所以把这两个使用|合并起来，一起跟QKV计算即可，否则得计算两次。</span>
    <span class="c1"># 对于encode来说传参只会穿key_pandding_mask，另外一个没有</span>
    <span class="c1"># 对于decoder来说，两个都会传递。</span>
    <span class="k">def</span> <span class="nf">_build_attn_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">Lq</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">Lk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""将两类掩码合并成 (1/ B, 1/ H, Lq, Lk) 可广播布尔张量。True 表示屏蔽。"""</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># (Lq, Lk) -&gt; (1,1,Lq,Lk)</span>
            <span class="n">m1</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">m1</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">mask</span> <span class="o">|</span> <span class="n">m1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># (B, Lk) -&gt; (B,1,1,Lk)</span>
            <span class="n">m2</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">m2</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">mask</span> <span class="o">|</span> <span class="n">m2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""多头注意力前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - query, key, value: (B, L, C)</span>
<span class="sd">          - attn_mask: (Lq, Lk) 因果/结构掩码，True 为屏蔽</span>
<span class="sd">          - key_padding_mask: (B, Lk) padding 掩码，True 为 padding</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, Lq, C)</span>
<span class="sd">        """</span>
        <span class="c1"># 输入均为 (B, L, C)</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">device</span>

        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_q</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>  <span class="c1"># (B,H,Lq,Dh)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_k</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>    <span class="c1"># (B,H,Lk,Dh)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_v</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># (B,H,Lk,Dh)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_attn_mask</span><span class="p">(</span><span class="n">Lq</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># (B,H,Lq,Dh)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>          <span class="c1"># (B,Lq,C)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""前馈网络：逐位置的两层 MLP（含激活与 dropout）"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""两层逐位置前馈网络。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, L, C)</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, L, C)</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Transformer 编码层（后归一化 post-norm 版本）</span>
<span class="sd">    子层：自注意力 + 前馈；均带残差连接与 LayerNorm。</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""单层编码层前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, S, C)</span>
<span class="sd">          - src_key_padding_mask: (B, S) True 为 padding</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, S, C)</span>
<span class="sd">        """</span>
        <span class="c1"># 自注意力子层</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_out</span><span class="p">)</span>
        <span class="c1"># 前馈子层</span>
        <span class="n">ff_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Transformer 解码层（自注意力 + 交叉注意力 + 前馈）"""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">memory</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">tgt_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memory_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""单层解码层前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, T, C) 解码器输入</span>
<span class="sd">          - memory: (B, S, C) 编码器输出</span>
<span class="sd">          - tgt_mask: (T, T) 因果掩码,true为屏蔽</span>
<span class="sd">          - tgt_key_padding_mask: (B, T)</span>
<span class="sd">          - memory_key_padding_mask: (B, S)</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, T, C)</span>
<span class="sd">        """</span>
        <span class="c1"># 1) 解码器自注意力（带因果掩码 tgt_mask）</span>
        <span class="n">sa</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">tgt_key_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">sa</span><span class="p">)</span>
        <span class="c1"># 2) 交叉注意力：Q 来自解码器，K/V 来自编码器 memory</span>
        <span class="n">ca</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">memory_key_padding_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ca</span><span class="p">)</span>
        <span class="c1"># 3) 前馈</span>
        <span class="n">ff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ff</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""堆叠若干编码层。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, S, C)</span>
<span class="sd">          - src_key_padding_mask: (B, S) True 为 padding</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, S, C)</span>
<span class="sd">        """</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">memory</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">tgt_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tgt_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memory_key_padding_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""堆叠若干解码层。</span>
<span class="sd">        参数：</span>
<span class="sd">          - x: (B, T, C) 目标端嵌入</span>
<span class="sd">          - memory: (B, S, C) 编码器输出</span>
<span class="sd">          - tgt_mask: (T, T) 因果掩码，True 为屏蔽</span>
<span class="sd">          - tgt_key_padding_mask: (B, T) 目标端 padding 掩码</span>
<span class="sd">          - memory_key_padding_mask: (B, S) 源端 padding 掩码</span>
<span class="sd">        返回：</span>
<span class="sd">          - (B, T, C)</span>
<span class="sd">        """</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">memory</span><span class="p">,</span>
                <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">,</span>
                <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_key_padding_mask</span><span class="p">,</span>
                <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">memory_key_padding_mask</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Seq2SeqTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""最小可运行的手写 Transformer 序列到序列模型</span>
<span class="sd">    - 使用我们实现的 Encoder/Decoder/MHA/FFN</span>
<span class="sd">    - 仍保持与上文训练/解码接口一致</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                 <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_tok</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">PAD_IDX</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_tok</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">PAD_IDX</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dim_ff</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_subsequent_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sz</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""构造大小为 (sz, sz) 的下三角因果掩码；True 为屏蔽（不允许看未来）。"""</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""训练/教师强制阶段的前向。</span>
<span class="sd">        参数：</span>
<span class="sd">          - src: (B, S) 源 token id</span>
<span class="sd">          - tgt_in: (B, T) 目标端输入（以 BOS 开头）</span>
<span class="sd">          - src_pad_mask: (B, S) True 为 padding</span>
<span class="sd">          - tgt_pad_mask: (B, T) True 为 padding（针对 tgt_in）</span>
<span class="sd">        返回：</span>
<span class="sd">          - logits: (B, T, V) 词表维度的分类分布</span>
<span class="sd">        """</span>
        <span class="c1"># 1) 词嵌入 + 位置编码</span>
        <span class="n">src_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_tok</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>  <span class="c1"># (B,S,C)</span>
        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_tok</span><span class="p">(</span><span class="n">tgt_in</span><span class="p">))</span>  <span class="c1"># (B,T,C)</span>

        <span class="c1"># 2) 编码：仅使用 key_padding_mask 屏蔽 padding</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_emb</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">)</span>  <span class="c1"># (B,S,C)</span>

        <span class="c1"># 3) 解码：自注意力需要因果掩码 + padding 掩码；交叉注意力需要 memory 的 padding 掩码</span>
        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_subsequent_mask</span><span class="p">(</span><span class="n">tgt_in</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># (T,T)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
            <span class="n">tgt_emb</span><span class="p">,</span>
            <span class="n">memory</span><span class="p">,</span>
            <span class="n">tgt_mask</span><span class="o">=</span><span class="n">tgt_mask</span><span class="p">,</span>
            <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_pad_mask</span><span class="p">,</span>
            <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># (B,T,C)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">logits</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">greedy_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""在推理阶段进行贪心解码。</span>
<span class="sd">        参数：</span>
<span class="sd">          - src_ids: 源端 token id 序列（不含 BOS/EOS）</span>
<span class="sd">          - max_len: 最大生成长度（含 BOS/EOS）</span>
<span class="sd">          - device: 运行设备</span>
<span class="sd">        返回：</span>
<span class="sd">          - 生成的目标端 id 序列（含 BOS/EOS）</span>
<span class="sd">        """</span>
        <span class="c1">#切换为评估模式，关闭dropout/batchnorm等随机性</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># 将源端token id序列转换为张量，并添加一个维度，如[1, 2, 3, 4] -&gt; [[1, 2, 3, 4]]</span>
        <span class="c1"># 变为批维度的 (1, S)；dtype 为 long 主要是以适配 nn.Embedding的输入格式。</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 生成一个跟src相同形状的mask矩阵，让编码器不要计算提取pandding的位置信息。</span>
        <span class="c1">#按元素判断 src 是否等于 PAD_IDX，等于的位置为 True，不等的位置为 False。</span>
        <span class="n">src_pad_mask</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>

        <span class="c1"># 计算src_tok= src 经过词嵌入+位置编码后的结果</span>
        <span class="n">src_tok</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_tok</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">src_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="n">src_tok</span><span class="p">)</span>

        <span class="c1"># 将该结果送入编码器,返回的memory就是编码器提取的特征向量。</span>
        <span class="c1"># 输入编码器，即使没有填充(pandding)的token,也需要传入src_key_padding_mask。</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_pos</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">)</span>

        <span class="c1"># 初始化目标端token id序列，维度为(1,1)，初始值为BOS_IDX</span>
        <span class="c1"># 表示目标端序列的开始，BOS_IDX=1</span>
        <span class="c1"># 推理时输入是没有PAD，但是仍然需要tgt_pad_mask.</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">BOS_IDX</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1">#计算本次解码的Mask，跟ys形状一样。</span>
            <span class="n">tgt_pad_mask</span> <span class="o">=</span> <span class="n">ys</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">PAD_IDX</span><span class="p">)</span>
            <span class="c1"># 计算本次因果掩码，把未来看到的token都屏蔽。</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_subsequent_mask</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># 可以看到当推理模式时，解码器输入token数量依次是1，2，3，4.....</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pos_enc</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_tok</span><span class="p">(</span><span class="n">ys</span><span class="p">)),</span>
                <span class="n">memory</span><span class="p">,</span>
                <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_pad_mask</span><span class="p">,</span>
                <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">src_pad_mask</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># 转化为预测词的概率分布</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>

            <span class="c1"># 使用贪心选择概率最大的作为本次预测的目标</span>
            <span class="n">next_token</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">next_id</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># 显示选择的token</span>
            <span class="n">token_text</span> <span class="o">=</span> <span class="n">TGT_ITOS</span><span class="p">[</span><span class="n">next_id</span><span class="p">]</span> <span class="k">if</span> <span class="n">next_id</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">TGT_ITOS</span><span class="p">)</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">"ID_</span><span class="si">{</span><span class="n">next_id</span><span class="si">}</span><span class="s2">"</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"选择: </span><span class="si">{</span><span class="n">token_text</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">next_id</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>

            <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ys</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_id</span> <span class="o">==</span> <span class="n">EOS_IDX</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="n">ys</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># --------------------------</span>
<span class="c1"># 4) Train</span>
<span class="c1"># --------------------------</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ToyDataset</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Seq2SeqTransformer</span><span class="p">(</span>
    <span class="n">src_vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">SRC_ITOS</span><span class="p">),</span>
    <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">TGT_ITOS</span><span class="p">),</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim_ff</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">PAD_IDX</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate_sample</span><span class="p">(</span><span class="n">sent</span><span class="o">=</span><span class="s2">"我 有 一个 苹果"</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""辅助函数：对输入中文句子进行编码→推理→解码并打印结果。"""</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">encode_src</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"ids"</span><span class="p">,</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">pred_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">greedy_decode</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">pred_text</span> <span class="o">=</span> <span class="n">decode_tgt</span><span class="p">(</span><span class="n">pred_ids</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'INPUT : </span><span class="si">{</span><span class="n">sent</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'OUTPUT: </span><span class="si">{</span><span class="n">pred_text</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Before training:"</span><span class="p">)</span>
<span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">)</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">80</span> <span class="c1"># 小步数即可过拟合玩具数据</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">tgt_out</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tgt_in</span> <span class="o">=</span> <span class="n">tgt_in</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tgt_out</span> <span class="o">=</span> <span class="n">tgt_out</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">src_pad_mask</span> <span class="o">=</span> <span class="n">src_pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tgt_pad_mask</span> <span class="o">=</span> <span class="n">tgt_pad_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt_in</span><span class="p">,</span> <span class="n">src_pad_mask</span><span class="p">,</span> <span class="n">tgt_pad_mask</span><span class="p">)</span>  <span class="c1"># (B, T, V)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tgt_out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2"> | loss=</span><span class="si">{</span><span class="n">total_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"After training:"</span><span class="p">)</span>
<span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"我 有 一个 苹果"</span><span class="p">)</span>
<span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"我 有 一本 书"</span><span class="p">)</span>
<span class="n">evaluate_sample</span><span class="p">(</span><span class="s2">"你 有 一个 苹果"</span><span class="p">)</span>
</code></pre></div></div>
  <div class="post-nav">
    <a class="prev" href="一步步实现transformer.html">← 一步步实现transformer</a>
    <a class="next" href="数据维度.html">数据维度 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="assets/site.js"></script>
  </body>
  </html>

