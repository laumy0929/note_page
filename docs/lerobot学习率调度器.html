<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>lerobot学习率调度器 - Laumy的技术栈</title>
    <link rel="stylesheet" href="assets/style.css">
    <!-- MathJax支持LaTeX数学公式 -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container">
        <a class="logo" href="./">Laumy的技术栈</a>
        <div class="search">
          <input id="search-input" type="text" placeholder="输入关键词回车搜索">
        </div>
        <div class="theme-toggle" title="切换主题" id="theme-toggle">☾</div>
        <nav class="top-nav">
          <div class="nav-item"><a href="./">首页</a></div>
          <div class="nav-item site-link">
            <a href="https://www.laumy.tech" target="_blank" title="访问主站">主站点:www.laumy.tech</a>
          </div>
        </nav>
      </div>
    </header>

    <main class="container layout">
      <aside class="left-nav">
        
        <div class="card">
          <div class="card-title">文章目录</div>
          <nav id="toc"><ul><li><a href="#_1">学习率调度器简介</a><ul><li><a href="#_2">是什么</a></li><li><a href="#_3">怎么用</a></li></ul></li><li><a href="#lerobot">lerobot调度器</a><ul><li><a href="#_4">抽象基类</a></li><li><a href="#_5">实例化子类</a></li><li><a href="#_6">状态管理</a></li></ul></li><li><a href="#_7">工程调用</a><ul><li><a href="#_8">命令参数</a></li><li><a href="#_9">创建</a></li><li><a href="#_10">更新</a></li><li><a href="#_11">续训恢复</a></li></ul></li></ul></nav>
        </div>
        
      </aside>

      <section class="content">
        
<article class="card post">
  <h1>lerobot学习率调度器</h1>
  <div class="meta">
    <span class="meta-item">
      <i class="icon">🕒</i>
      2025-08-02
    </span>
    <span class="meta-item">
      <i class="icon">📂</i>
      ai
    </span>
    <span class="meta-item">
      <i class="icon">👤</i>
      laumy
    </span>
  </div>
  <div class="post-content"><h2 id="_1">学习率调度器简介</h2>
<h3 id="_2">是什么</h3>
<p>学习率调度器（Learning Rate Scheduler）是深度学习训练中动态调整<strong>优化器学习率的工具</strong>（注意是在优化器的基础上动态调整学习率），通过优化收敛过程提升模型性能。</p>
<p>学习率（η）控制梯度更新步长，参数更新量 = -η × 梯度。过大步长导致震荡，过小则陷入局部最优或训练缓慢，固定学习率易导致训练初期震荡或后期收敛缓慢，调度器在训练期间自适应调整初期较高学习率加速收敛，中期稳定探索最优解，后期精细调优避免震荡。</p>
<p>以下是常见的学习率调度器</p>
<ul>
<li>StepLR：每隔固定epoch将学习率乘以衰减因子（如step_size=10, gamma=0.5）。</li>
<li>ExponentialLR：每epoch按指数衰减（lr = lr × gamma）。</li>
<li>CosineAnnealingLR：按余弦曲线周期下降：η = η_min + 0.5×(η_max - η_min)×(1 + cos(π×t/T_max))。</li>
<li>OneCycleLR：分两阶段：线性升温至峰值，再退火至极小值。</li>
<li>PowerLR：基于幂律关系调整，与批次大小和token数量无关。</li>
</ul>
<h3 id="_3">怎么用</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">StepLR</span>

<span class="c1"># 1. 定义模型和优化器</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># 神经网络模型</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># 初始学习率0.1</span>

<span class="c1"># 2. 创建调度器（绑定优化器）</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># 每30epoch衰减50%</span>

<span class="c1"># 3. 训练循环</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>

    <span class="c1"># 前向传播 + 反向传播</span>
    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 4. 更新学习率（通常在epoch结束时）</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 查看当前学习率</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: LR=</span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p>上面代码演示了基础的学习率调度使用示例，创建一个学习率调度器，传入的参数有优化器，主要的目的是让学习率调度器和优化器绑定，因为相当于是在优化器的基础上改进学习率调度。接下来就是在前向、反向传播更新完参数后调用scheduler.step()更新学习率，以便下一次计算使用。</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingLR</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># 基础学习率</span>
<span class="n">total_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">warmup_epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 预热epoch数</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_epochs</span><span class="p">):</span>
    <span class="c1"># 1. 预热阶段：前10epoch线性增长</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="n">warmup_epochs</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="n">warmup_epochs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="c1"># 2. 余弦退火阶段</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_epochs</span><span class="o">-</span><span class="n">warmup_epochs</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 训练代码同上...</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: LR=</span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'lr'</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<p>上面使用的是余弦退火+预热的方式。</p>
<h2 id="lerobot">lerobot调度器</h2>
<h3 id="_4">抽象基类</h3>
<div class="codehilite"><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LRSchedulerConfig</span><span class="p">(</span><span class="n">draccus</span><span class="o">.</span><span class="n">ChoiceRegistry</span><span class="p">,</span> <span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
    <span class="n">num_warmup_steps</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># 预热步数（所有调度器的公共参数）</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_choice_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>  <span class="c1"># 获取子类注册名称（如 "diffuser"）</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LRScheduler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""创建学习率调度器实例"""</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div>
<p>LRSchedulerConfig是学习率调度器配置的核心抽象，通过抽象接口定义+配置注册机制，实现了学习率调度策略的统一管理与灵活扩展。</p>
<p>其同样继承了abc.ABC标记为抽象基类，强制子类事项build抽象方法，同时继承draccus.ChoiceRegistry提供子类注册机制，通过 @LRSchedulerConfig.register_subclass("名称") 将子类与调度器类型绑定（如 "diffuser" → DiffuserSchedulerConfig），支持配置驱动的动态实例化。同时使用了@dataclass 装饰器自动生成构造函数、<strong>repr</strong> 等方法，简化调度器超参数的定义与管理。</p>
<p>其核心属性只有一个num_warmup_steps表示学习率预热步数。预热是深度学习训练的常见技巧（尤其在 Transformer 等模型中），将其作为基类字段可避免子类重复定义。也是几乎所有学习率调度器的基础参数，用于控制“预热阶段”（学习率从低到高线性增长的步数），避免训练初期因高学习率导致的不稳定。</p>
<p>其只有两个方法type和build，type是用于获取子类注册调度器类型名称进而匹配到对应子类，而build的方法是强制子类实现。</p>
<h3 id="_5">实例化子类</h3>
<p>lerobot的学习率调度实现了3个DiffuserSchedulerConfig、VQBeTSchedulerConfig、CosineDecayWithWarmupSchedulerConfig子类，这里以DiffuserSchedulerConfig简单说明。</p>
<div class="codehilite"><pre><span></span><code><span class="nd">@LRSchedulerConfig</span><span class="o">.</span><span class="n">register_subclass</span><span class="p">(</span><span class="s2">"diffuser"</span><span class="p">)</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DiffuserSchedulerConfig</span><span class="p">(</span><span class="n">LRSchedulerConfig</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"cosine"</span>  <span class="c1"># 调度器类型（如 "cosine"、"linear"，来自 diffusers）</span>
    <span class="n">num_warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 预热步数（可选，未指定则不预热）</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LambdaLR</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">diffusers.optimization</span> <span class="kn">import</span> <span class="n">get_scheduler</span>  <span class="c1"># 复用 Diffusers 的调度器实现</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="s2">"num_training_steps"</span><span class="p">:</span> <span class="n">num_training_steps</span><span class="p">,</span> <span class="s2">"optimizer"</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">}</span> <span class="c1"># 构造调度器参数：类字段 + 训练相关参数</span>
        <span class="k">return</span> <span class="n">get_scheduler</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># 调用 diffusers API 创建调度器</span>
</code></pre></div>
<p>diffusers.optimization.get_scheduler 是 Diffusers 库提供的调度器工厂函数，支持多种预定义策略（如 "cosine"、"linear"、"constant" 等）。</p>
<p>asdict(self)将 DiffuserSchedulerConfig 实例的字段（如 name="cosine"、num_warmup_steps=1000）转换为字典；而num_training_steps：总训练步数（调度器需基于此计算衰减周期）；最后的optimizer就是待绑定的优化器实例（调度器需调整其参数组的学习率）。</p>
<h3 id="_6">状态管理</h3>
<p><strong>（1）存储</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">save_scheduler_state</span><span class="p">(</span><span class="n">scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>  <span class="c1"># 获取调度器状态（如当前 step、预热步数等）</span>
    <span class="n">write_json</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">save_dir</span> <span class="o">/</span> <span class="n">SCHEDULER_STATE</span><span class="p">)</span>  <span class="c1"># 保存为 JSON 文件（如 "scheduler_state.json"）</span>
</code></pre></div>
<p>该函数主要是将学习率调度器的状态写入到磁盘，为后续断点续训提供支持。state_dict = scheduler.state_dict()获取调度器的当前状态字典，包含调度器运行所需的所有动态信息。接着调用write_json写入到到文件中，文件名默认scheduler_state.json。</p>
<p><strong>（2）加载</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">load_scheduler_state</span><span class="p">(</span><span class="n">scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LRScheduler</span><span class="p">:</span>
    <span class="c1"># 从 JSON 加载状态，并按当前调度器结构适配（确保兼容性）</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">deserialize_json_into_object</span><span class="p">(</span><span class="n">save_dir</span> <span class="o">/</span> <span class="n">SCHEDULER_STATE</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>  <span class="c1"># 恢复状态</span>
    <span class="k">return</span> <span class="n">scheduler</span>
</code></pre></div>
<p>该函数也比较简单，主要负责从磁盘加载之前保存的调度器状态（如当前训练步数、学习率调整进度等），使训练能够从断点处继续，确保学习率调整策略的连续性。</p>
<h2 id="_7">工程调用</h2>
<h3 id="_8">命令参数</h3>
<p>在lerobot中，启动训练是可以通过参数来实例化使用哪些调度器，如下。</p>
<ul>
<li>--scheduler.type=diffuser指定使用diffuser调度类。</li>
<li>--scheduler.num_warmup_steps=1000指定预热步数。</li>
<li>--scheduler.num_warmup_steps=0.5指定余弦衰减周期。</li>
</ul>
<h3 id="_9">创建</h3>
<p>工程中通过 optim/factory.py 中的 make_optimizer_and_scheduler 函数统一创建优化器和调度器，并完成两者的绑定。</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">make_optimizer_and_scheduler</span><span class="p">(</span>
    <span class="n">cfg</span><span class="p">:</span> <span class="n">TrainPipelineConfig</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="n">PreTrainedPolicy</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">LRScheduler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="c1"># 1. 创建优化器（基于优化器配置，如 AdamConfig、MultiAdamConfig）</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>  <span class="c1"># policy.parameters() 为模型参数</span>

    <span class="c1"># 2. 创建调度器（基于调度器配置，如 DiffuserSchedulerConfig、VQBeTSchedulerConfig）</span>
    <span class="c1"># 关键：通过调度器配置的 `build` 方法，将优化器作为参数传入，完成绑定。</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">steps</span><span class="p">)</span> <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_scheduler</span>
</code></pre></div>
<ul>
<li>优化器创建：cfg.optimizer.build(...) 根据配置生成优化器实例（如 Adam、AdamW），并关联模型参数（policy.parameters()）。</li>
<li>调度器绑定：cfg.scheduler.build(optimizer, cfg.steps) 调用调度器配置类（如 DiffuserSchedulerConfig）的 build 方法，将优化器实例作为参数传入，生成与该优化器绑定的调度器实例（如 LambdaLR）</li>
</ul>
<h3 id="_10">更新</h3>
<p>在训练过程中，如工程 scripts/train.py 中，调度器被集成到训练循环，通过 scheduler.step() 更新学习率通过调用scheduler.step() 更新学习率。</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="c1"># ... 初始化优化器、调度器 ...</span>
    <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">make_optimizer_and_scheduler</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">policy</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_step</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">steps</span><span class="p">):</span>
        <span class="c1"># ... 前向传播、损失计算、反向传播 ...</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># 调用调度器更新学习率</span>
</code></pre></div>
<h3 id="_11">续训恢复</h3>
<p>在断点续训是，通过 utils/train_utils.py 中的 save_training_state 和 load_training_state 函数处理断点续训，其中调用了 schedulers.py 的状态管理函数：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">save_training_state</span><span class="p">(</span><span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
    <span class="c1"># ... 保存优化器状态 ...</span>
    <span class="k">if</span> <span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">save_scheduler_state</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>  <span class="c1"># 调用 schedulers.py 中的保存函数</span>

<span class="k">def</span> <span class="nf">load_training_state</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">):</span>
    <span class="c1"># ... 加载优化器状态 ...</span>
    <span class="k">if</span> <span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">load_scheduler_state</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>  <span class="c1"># 调用 schedulers.py 中的加载函数</span>
    <span class="k">return</span> <span class="n">step</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_scheduler</span>
</code></pre></div></div>
  <div class="post-nav">
    <a class="prev" href="具身智能act算法.html">← 具身智能ACT算法</a>
    <a class="next" href="lerobot策略优化器.html">lerobot策略优化器 →</a>
  </div>
</article>

      </section>

      <aside class="right-panel">
        
      </aside>
    </main>

    <footer class="site-footer">
      <div class="container">Copyright ©2022-2025 laumy 版权所有</div>
    </footer>

    <script src="assets/site.js"></script>
  </body>
  </html>

